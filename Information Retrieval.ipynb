{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# All of the irrelevent words and punctuation to be removed from document texts\n",
    "stop_words = stopwords.words('english')\n",
    "DELIM = '[ \\n\\t0123456789;:.,&$£/#~@><|!%^*\\(\\)\\\"\\'-]+'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads a document with the index docid \n",
    "\n",
    "def readfile(path, docid):\n",
    "    files = sorted(os.listdir(path))\n",
    "    f = open(os.path.join(path, files[docid]), 'r',encoding='latin-1')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizes each of the words so that words can be easily\n",
    "# inspected. Text is all converted to lower case too to\n",
    "# prevent multiple variations of the same word being tokenized\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.split(DELIM, text.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The values for the terms in all documents are calculated\n",
    "# so that they can be accessed using an index later with a query\n",
    "\n",
    "def indextextfiles_RR(path):\n",
    "    \n",
    "    N = len(sorted(os.listdir(path)))                      # The number of documents\n",
    "    postings = {}                                          # The documents which the word appears in\n",
    "    df = {}                                                # Document Frequency \n",
    "    idf_in_all_docs = {}                                   # All IDF values for each of the words  \n",
    "    tf_in_all_docs = []                                    # Term frequencies for words in all documents \n",
    "    list_of_normTF=[]                                      # The list of all documents normalised term frequencies\n",
    "    final_docCollection = []                               # The final collection of all tfidf values for every term\n",
    "    uniqueWords = {}                                       # The set used to check whether a query word is in any document\n",
    "    \n",
    "    \n",
    "    for docID in range(N):                                 # Reads each file one by one in the collection of documents\n",
    "        s = readfile(path, docID)                           \n",
    "        words = sorted(tokenize(s))                        # Tokenises each of the words in the document\n",
    "        words = [w for w in words if not w in stop_words]  # Removes all stopwords such as I, a, you... as they are irrelevent \n",
    "        specificDocWords = {}                              # Dictionary of all unique words in the document\n",
    "        normTF = {}                                        # Dictionary of all the normalised term frequencies in a document\n",
    "            \n",
    "        for w in words:\n",
    "            if w!='':\n",
    "                postings.setdefault(w, set()).add(docID)   # Adds each of the docIDs to a set containng the words          \n",
    "                \n",
    "                # Calculates the unique words and their frequencies in individual documents\n",
    "                if w in specificDocWords.keys():\n",
    "                    specificDocWords[w] = specificDocWords[w] + 1\n",
    "                else:\n",
    "                    specificDocWords[w] = 1 \n",
    "           \n",
    "            if w not in uniqueWords.keys():\n",
    "                uniqueWords[w] = 1 \n",
    "    \n",
    "        \n",
    "        # calculates the normalised tf for each unique word in the document\n",
    "        words_in_doc_Count = len(words)\n",
    "        for word in specificDocWords.keys():\n",
    "            normTF[word] = (specificDocWords[word] / words_in_doc_Count)               \n",
    "        \n",
    "        \n",
    "        # Appends all of the dictionaries with normalised tf into one list\n",
    "        tf_in_all_docs.append(normTF)    \n",
    "    \n",
    "    \n",
    "    # gets the df values for all terms \n",
    "    # -- The number of documents each term appears in \n",
    "    # gets the idf values for all terms using the df values\n",
    "    # -- Log10 ( number of documents / df ) \n",
    "    for x in postings.keys():\n",
    "        df[x] = len(postings[x])\n",
    "        idf_in_all_docs[x] = np.log10(N / df[x])\n",
    "        \n",
    "       \n",
    "    # tfidf calculated for each of the words in a document by multiplying\n",
    "    # the idf values of terms by their normalised term frequencies which \n",
    "    # are stored in the final document \n",
    "    for docID in range(N): \n",
    "        list_of_normTF = list(tf_in_all_docs[docID].items())\n",
    "        tfidf = {}\n",
    "\n",
    "        for index in range(len(list_of_normTF)):\n",
    "            ind = list_of_normTF[index]\n",
    "            word = ind[0]\n",
    "            tfidf[word] = ind[1] * idf_in_all_docs[word] \n",
    "\n",
    "        final_docCollection.append(tfidf)\n",
    "    \n",
    "    \n",
    "    return postings, N, final_docCollection, uniqueWords, specificDocWords, tf_in_all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_RR(postings, qtext):\n",
    "    words = tokenize(qtext)                                    # Query Tokenised and all stopwords removed as they are \n",
    "    words = [w for w in words if not w in stop_words]          # not calculated in the index due to lack of relevence.\n",
    "    score_of_documents = {}                                    # Dictionary contains docIDs as keys and scores as values\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in uniqueWords.keys():\n",
    "            return print(\"The word \\'\" + w + \"\\' cannot be found in any of the documents! \\nRemove this word from the query and try again.\")\n",
    "        \n",
    "    allpostings = [postings[w] for w in words if w!='']        # The postings for each individual word in the query\n",
    "\n",
    "    \n",
    "    # Loops through all of the postings which the words in \n",
    "    # the query can be found in\n",
    "    for wordDocs in range(len(allpostings)):\n",
    "\n",
    "        \n",
    "        # The postings of each individual word in the query\n",
    "        collectionOfDocs = allpostings[wordDocs]\n",
    "\n",
    "        \n",
    "        # Loops through each of the documents which contain the words \n",
    "        for docID in collectionOfDocs:\n",
    "            document = final_docCollection[docID]\n",
    "            score = 0\n",
    "\n",
    "            # Looping through each of the words in the query 1 at a time, retrieving\n",
    "            # the tfidf values of the words\n",
    "            for queryWords in words:\n",
    "                if queryWords in document.keys():\n",
    "                    score += document[queryWords]\n",
    "                    \n",
    "\n",
    "            # Each of the query words are added to a dictionary as their keys\n",
    "            # and their final scores are the values.\n",
    "            score_of_documents[docID] = score\n",
    "            \n",
    "    \n",
    "    # Orders all of the scores with the highest ranking document appearing first.\n",
    "    # Only the top 10 relevent documents are outputted for efficiency\n",
    "    # Reversed so that the highest ranking document is the first displayed \n",
    "    final_rankings = {}\n",
    "    sorted_keys = sorted(score_of_documents, key=score_of_documents.get, reverse=True)\n",
    "\n",
    "    for w in sorted_keys[:10]:\n",
    "        final_rankings[w] = score_of_documents[w]\n",
    "    \n",
    "    \n",
    "    return print(\"The top 10 relevent documents with their scores for the given query are: \\n\\n\" + str(final_rankings))\n",
    "    #return print(\"The top 10 relevent documents with their scores for the given query are: \\n\\n\" + str(final_rankings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 relevent documents with their scores for the given query are: \n",
      "\n",
      "{372: 0.09078423131156231, 736: 0.08696477174635121, 446: 0.08347265659341374, 294: 0.07990606677483257, 462: 0.06852744216405984, 4: 0.06643204207432916, 36: 0.05810832979076925, 80: 0.05746696492744416, 91: 0.056897343847774895, 177: 0.05162035849800805}\n"
     ]
    }
   ],
   "source": [
    "postings, N, final_docCollection, uniqueWords, specificDocWords, tf_in_all_docs = indextextfiles_RR('docs')\n",
    "query_RR(postings,'christmas champions mourinho arsenal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing of the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lack of Christmas spirit\n",
      "\n",
      "It's that time of year when footballers and managers brace themselves for what I think is the most important period of the entire season.\n",
      "\n",
      "I was thinking to myself last week that the last time I had a Christmas off was 39 years ago. I have never been out of work at Christmas as a player or manager since I was 17 when our youth team coach at Chesterfield, a chap called Reg Wright, gave us Christmans off. But only because there were no games. I think things have changed dramatically over the years in terms of discipline and looking after themselves. Players take a lot more responsibility these days, in particular the older ones - I'm talking about those 32 and over, here. They've changed their whole outlook in order to continue playing at this level. Managers as well need to trust players more than we have in the past. In my squad I haven't got anyone I have to warn regarding excess and over-eating, which is a massive bonus.\n",
      "\n",
      "Over the years, there have been some players in the squad who you would never know if they were going to turn up for training smelling of booze. As per usual, we will be training on Christmas Day, prior to our Boxing Day trip to Coventry. But there are times when you can do too much over Christmas, having the players in for training and then leaving for the game. I'll try and strike a balance and after we've trained in the morning, the players can go home for a few hours and we'll leave for Coventry about 7pm. I allow the players to have a pre-Christmas night out. They came to me in November and asked if they could have a night out in Leeds and I said 'no'. I also said 'no' to Manchester, Sheffield and Nottingham and eventually let them go to Dublin after we played at Millwall. I send a minder with them to look after them, not because I don't trust them.\n",
      "\n",
      "The problem is that nowadays, footballers are big news and you never know when somebody is going to step out with a mobile phone and take pictures of them. You have to trust your players to behave themselves, but unfortunately you can't govern for other people's behaviour. There is always an idiot out there who wants to get himself a bit notoriety or his name in the local paper by picking a fight and taking a pop at a professional footballer. I also know that last year one newspaper asked certain young ladies to find out when and where players were holding their Christmas parties in the hope of getting embarrassing photos. I tried to behave myself as a player and I remember when I was at Scunthorpe, going to bed at 10pm on New Year's Eve as we had a game on New Year's Day.\n",
      "\n",
      "I missed all the festivities and seeing the new year in, only to wake up the next morning to find there was two feet of snow on the ground! We all love Christmas, though and I like to take my children William and Amy to see the lights and take them to Santa's grotto. But when you're in football you accept that Christmas is not the same for you as for others. In fact, until somebody mentioned it to me the other day, I never realised that it's become the norm for me and others in football not to have a Christmas holiday. One of the nice things when I do retire will be not having to worry about the phone ringing over Christmas. You're always on tenterhooks on Christmas Day that somebody is injured or has had an accident playing with their kids. But I would like to take this opportunity to wish everybody a Happy Christmas and a prosperous new year.\n",
      "\n",
      "\n",
      "The number of words in the document when stop words are removed: \n",
      "286\n"
     ]
    }
   ],
   "source": [
    "s = readfile('docs', 736)\n",
    "print(s)\n",
    "words = sorted(tokenize(s))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "\n",
    "print(\"\\nThe number of words in the document when stop words are removed: \\n\" + str(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw term frequencies for the last document in the collection: \n",
      "\n",
      "{'accept': 1, 'accident': 1, 'ago': 1, 'allow': 1, 'also': 2, 'always': 2, 'amy': 1, 'anyone': 1, 'asked': 2, 'balance': 1, 'become': 1, 'bed': 1, 'behave': 2, 'behaviour': 1, 'big': 1, 'bit': 1, 'bonus': 1, 'booze': 1, 'boxing': 1, 'brace': 1, 'called': 1, 'came': 1, 'certain': 1, 'changed': 2, 'chap': 1, 'chesterfield': 1, 'children': 1, 'christmans': 1, 'christmas': 13, 'coach': 1, 'continue': 1, 'could': 1, 'coventry': 2, 'day': 5, 'days': 1, 'discipline': 1, 'dramatically': 1, 'dublin': 1, 'eating': 1, 'embarrassing': 1, 'entire': 1, 'eve': 1, 'eventually': 1, 'everybody': 1, 'excess': 1, 'fact': 1, 'feet': 1, 'festivities': 1, 'fight': 1, 'find': 2, 'football': 2, 'footballer': 1, 'footballers': 2, 'game': 2, 'games': 1, 'gave': 1, 'get': 1, 'getting': 1, 'go': 2, 'going': 3, 'got': 1, 'govern': 1, 'grotto': 1, 'ground': 1, 'happy': 1, 'holding': 1, 'holiday': 1, 'home': 1, 'hope': 1, 'hours': 1, 'idiot': 1, 'important': 1, 'injured': 1, 'kids': 1, 'know': 3, 'lack': 1, 'ladies': 1, 'last': 3, 'leave': 1, 'leaving': 1, 'leeds': 1, 'let': 1, 'level': 1, 'lights': 1, 'like': 2, 'local': 1, 'look': 1, 'looking': 1, 'lot': 1, 'love': 1, 'manager': 1, 'managers': 2, 'manchester': 1, 'massive': 1, 'mentioned': 1, 'millwall': 1, 'minder': 1, 'missed': 1, 'mobile': 1, 'morning': 2, 'much': 1, 'name': 1, 'need': 1, 'never': 4, 'new': 4, 'news': 1, 'newspaper': 1, 'next': 1, 'nice': 1, 'night': 2, 'norm': 1, 'notoriety': 1, 'nottingham': 1, 'november': 1, 'nowadays': 1, 'older': 1, 'one': 2, 'ones': 1, 'opportunity': 1, 'order': 1, 'others': 2, 'outlook': 1, 'paper': 1, 'particular': 1, 'parties': 1, 'past': 1, 'people': 1, 'per': 1, 'period': 1, 'phone': 2, 'photos': 1, 'picking': 1, 'pictures': 1, 'played': 1, 'player': 2, 'players': 8, 'playing': 2, 'pm': 2, 'pop': 1, 'pre': 1, 'prior': 1, 'problem': 1, 'professional': 1, 'prosperous': 1, 'realised': 1, 'reg': 1, 'regarding': 1, 'remember': 1, 'responsibility': 1, 'retire': 1, 'ringing': 1, 'said': 2, 'santa': 1, 'scunthorpe': 1, 'season': 1, 'see': 1, 'seeing': 1, 'send': 1, 'sheffield': 1, 'since': 1, 'smelling': 1, 'snow': 1, 'somebody': 3, 'spirit': 1, 'squad': 2, 'step': 1, 'strike': 1, 'take': 5, 'taking': 1, 'talking': 1, 'team': 1, 'tenterhooks': 1, 'terms': 1, 'things': 2, 'think': 2, 'thinking': 1, 'though': 1, 'time': 2, 'times': 1, 'trained': 1, 'training': 3, 'tried': 1, 'trip': 1, 'trust': 3, 'try': 1, 'turn': 1, 'two': 1, 'unfortunately': 1, 'us': 1, 'usual': 1, 'wake': 1, 'wants': 1, 'warn': 1, 'week': 1, 'well': 1, 'whole': 1, 'william': 1, 'wish': 1, 'work': 1, 'worry': 1, 'would': 2, 'wright': 1, 'year': 6, 'years': 3, 'young': 1, 'youth': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"The raw term frequencies for the last document in the collection: \\n\\n\" + str(specificDocWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The normalised versions of the terms in the same document: \n",
      "\n",
      "{'accept': 0.0034965034965034965, 'accident': 0.0034965034965034965, 'ago': 0.0034965034965034965, 'allow': 0.0034965034965034965, 'also': 0.006993006993006993, 'always': 0.006993006993006993, 'amy': 0.0034965034965034965, 'anyone': 0.0034965034965034965, 'asked': 0.006993006993006993, 'balance': 0.0034965034965034965, 'become': 0.0034965034965034965, 'bed': 0.0034965034965034965, 'behave': 0.006993006993006993, 'behaviour': 0.0034965034965034965, 'big': 0.0034965034965034965, 'bit': 0.0034965034965034965, 'bonus': 0.0034965034965034965, 'booze': 0.0034965034965034965, 'boxing': 0.0034965034965034965, 'brace': 0.0034965034965034965, 'called': 0.0034965034965034965, 'came': 0.0034965034965034965, 'certain': 0.0034965034965034965, 'changed': 0.006993006993006993, 'chap': 0.0034965034965034965, 'chesterfield': 0.0034965034965034965, 'children': 0.0034965034965034965, 'christmans': 0.0034965034965034965, 'christmas': 0.045454545454545456, 'coach': 0.0034965034965034965, 'continue': 0.0034965034965034965, 'could': 0.0034965034965034965, 'coventry': 0.006993006993006993, 'day': 0.017482517482517484, 'days': 0.0034965034965034965, 'discipline': 0.0034965034965034965, 'dramatically': 0.0034965034965034965, 'dublin': 0.0034965034965034965, 'eating': 0.0034965034965034965, 'embarrassing': 0.0034965034965034965, 'entire': 0.0034965034965034965, 'eve': 0.0034965034965034965, 'eventually': 0.0034965034965034965, 'everybody': 0.0034965034965034965, 'excess': 0.0034965034965034965, 'fact': 0.0034965034965034965, 'feet': 0.0034965034965034965, 'festivities': 0.0034965034965034965, 'fight': 0.0034965034965034965, 'find': 0.006993006993006993, 'football': 0.006993006993006993, 'footballer': 0.0034965034965034965, 'footballers': 0.006993006993006993, 'game': 0.006993006993006993, 'games': 0.0034965034965034965, 'gave': 0.0034965034965034965, 'get': 0.0034965034965034965, 'getting': 0.0034965034965034965, 'go': 0.006993006993006993, 'going': 0.01048951048951049, 'got': 0.0034965034965034965, 'govern': 0.0034965034965034965, 'grotto': 0.0034965034965034965, 'ground': 0.0034965034965034965, 'happy': 0.0034965034965034965, 'holding': 0.0034965034965034965, 'holiday': 0.0034965034965034965, 'home': 0.0034965034965034965, 'hope': 0.0034965034965034965, 'hours': 0.0034965034965034965, 'idiot': 0.0034965034965034965, 'important': 0.0034965034965034965, 'injured': 0.0034965034965034965, 'kids': 0.0034965034965034965, 'know': 0.01048951048951049, 'lack': 0.0034965034965034965, 'ladies': 0.0034965034965034965, 'last': 0.01048951048951049, 'leave': 0.0034965034965034965, 'leaving': 0.0034965034965034965, 'leeds': 0.0034965034965034965, 'let': 0.0034965034965034965, 'level': 0.0034965034965034965, 'lights': 0.0034965034965034965, 'like': 0.006993006993006993, 'local': 0.0034965034965034965, 'look': 0.0034965034965034965, 'looking': 0.0034965034965034965, 'lot': 0.0034965034965034965, 'love': 0.0034965034965034965, 'manager': 0.0034965034965034965, 'managers': 0.006993006993006993, 'manchester': 0.0034965034965034965, 'massive': 0.0034965034965034965, 'mentioned': 0.0034965034965034965, 'millwall': 0.0034965034965034965, 'minder': 0.0034965034965034965, 'missed': 0.0034965034965034965, 'mobile': 0.0034965034965034965, 'morning': 0.006993006993006993, 'much': 0.0034965034965034965, 'name': 0.0034965034965034965, 'need': 0.0034965034965034965, 'never': 0.013986013986013986, 'new': 0.013986013986013986, 'news': 0.0034965034965034965, 'newspaper': 0.0034965034965034965, 'next': 0.0034965034965034965, 'nice': 0.0034965034965034965, 'night': 0.006993006993006993, 'norm': 0.0034965034965034965, 'notoriety': 0.0034965034965034965, 'nottingham': 0.0034965034965034965, 'november': 0.0034965034965034965, 'nowadays': 0.0034965034965034965, 'older': 0.0034965034965034965, 'one': 0.006993006993006993, 'ones': 0.0034965034965034965, 'opportunity': 0.0034965034965034965, 'order': 0.0034965034965034965, 'others': 0.006993006993006993, 'outlook': 0.0034965034965034965, 'paper': 0.0034965034965034965, 'particular': 0.0034965034965034965, 'parties': 0.0034965034965034965, 'past': 0.0034965034965034965, 'people': 0.0034965034965034965, 'per': 0.0034965034965034965, 'period': 0.0034965034965034965, 'phone': 0.006993006993006993, 'photos': 0.0034965034965034965, 'picking': 0.0034965034965034965, 'pictures': 0.0034965034965034965, 'played': 0.0034965034965034965, 'player': 0.006993006993006993, 'players': 0.027972027972027972, 'playing': 0.006993006993006993, 'pm': 0.006993006993006993, 'pop': 0.0034965034965034965, 'pre': 0.0034965034965034965, 'prior': 0.0034965034965034965, 'problem': 0.0034965034965034965, 'professional': 0.0034965034965034965, 'prosperous': 0.0034965034965034965, 'realised': 0.0034965034965034965, 'reg': 0.0034965034965034965, 'regarding': 0.0034965034965034965, 'remember': 0.0034965034965034965, 'responsibility': 0.0034965034965034965, 'retire': 0.0034965034965034965, 'ringing': 0.0034965034965034965, 'said': 0.006993006993006993, 'santa': 0.0034965034965034965, 'scunthorpe': 0.0034965034965034965, 'season': 0.0034965034965034965, 'see': 0.0034965034965034965, 'seeing': 0.0034965034965034965, 'send': 0.0034965034965034965, 'sheffield': 0.0034965034965034965, 'since': 0.0034965034965034965, 'smelling': 0.0034965034965034965, 'snow': 0.0034965034965034965, 'somebody': 0.01048951048951049, 'spirit': 0.0034965034965034965, 'squad': 0.006993006993006993, 'step': 0.0034965034965034965, 'strike': 0.0034965034965034965, 'take': 0.017482517482517484, 'taking': 0.0034965034965034965, 'talking': 0.0034965034965034965, 'team': 0.0034965034965034965, 'tenterhooks': 0.0034965034965034965, 'terms': 0.0034965034965034965, 'things': 0.006993006993006993, 'think': 0.006993006993006993, 'thinking': 0.0034965034965034965, 'though': 0.0034965034965034965, 'time': 0.006993006993006993, 'times': 0.0034965034965034965, 'trained': 0.0034965034965034965, 'training': 0.01048951048951049, 'tried': 0.0034965034965034965, 'trip': 0.0034965034965034965, 'trust': 0.01048951048951049, 'try': 0.0034965034965034965, 'turn': 0.0034965034965034965, 'two': 0.0034965034965034965, 'unfortunately': 0.0034965034965034965, 'us': 0.0034965034965034965, 'usual': 0.0034965034965034965, 'wake': 0.0034965034965034965, 'wants': 0.0034965034965034965, 'warn': 0.0034965034965034965, 'week': 0.0034965034965034965, 'well': 0.0034965034965034965, 'whole': 0.0034965034965034965, 'william': 0.0034965034965034965, 'wish': 0.0034965034965034965, 'work': 0.0034965034965034965, 'worry': 0.0034965034965034965, 'would': 0.006993006993006993, 'wright': 0.0034965034965034965, 'year': 0.02097902097902098, 'years': 0.01048951048951049, 'young': 0.0034965034965034965, 'youth': 0.0034965034965034965}\n"
     ]
    }
   ],
   "source": [
    "print(\"The normalised versions of the terms in the same document: \\n\\n\" + str(tf_in_all_docs[736]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of documents that contain the specific word 'years' \n",
      "175\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of documents that contain the specific word \\'years\\' \\n\" + str(len(postings['years'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All of the documents that contain the specific word 'years' \n",
      "\n",
      "{0, 5, 517, 7, 12, 15, 16, 527, 20, 22, 537, 541, 543, 33, 546, 547, 36, 37, 40, 553, 43, 45, 558, 559, 49, 52, 53, 565, 568, 575, 576, 66, 578, 68, 579, 70, 75, 76, 79, 80, 591, 82, 597, 603, 604, 94, 607, 613, 615, 617, 106, 108, 622, 623, 624, 113, 123, 124, 125, 636, 127, 129, 645, 646, 135, 649, 139, 140, 141, 655, 145, 146, 659, 662, 151, 664, 158, 672, 161, 162, 675, 676, 165, 679, 681, 170, 682, 683, 685, 691, 692, 183, 695, 185, 696, 188, 700, 190, 701, 196, 709, 710, 711, 201, 715, 206, 719, 723, 212, 725, 726, 727, 729, 218, 731, 732, 733, 736, 233, 236, 238, 241, 257, 258, 264, 269, 270, 271, 274, 277, 283, 292, 293, 304, 306, 310, 315, 318, 325, 329, 335, 336, 339, 341, 342, 348, 351, 361, 367, 369, 370, 375, 376, 383, 385, 386, 388, 391, 392, 393, 394, 397, 401, 407, 414, 424, 446, 447, 471, 483, 485, 486, 494, 503, 510}\n",
      "\n",
      "The final document in the collection, 'document 736' can be seen in here which is expected\n"
     ]
    }
   ],
   "source": [
    "print(\"All of the documents that contain the specific word \\'years\\' \\n\\n\" + str(postings['years']) + \"\\n\\nThe final document in the collection, \\'document 736\\' can be seen in here which is expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The idf value of the word 'years' is: \n",
      "0.6244294391727571\n"
     ]
    }
   ],
   "source": [
    "print(\"The idf value of the word \\'years\\' is: \\n\"+ str(np.log10(737 / len(postings['years']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tfidf values of the words in the final document: \n",
      "{'accept': 0.00613120327116159, 'accident': 0.0062527490972427505, 'ago': 0.003641233164888899, 'allow': 0.004811558720366359, 'also': 0.002679494045540319, 'always': 0.005947091947973924, 'amy': 0.008973557665017728, 'anyone': 0.005021341691258966, 'asked': 0.009254541562998432, 'balance': 0.006018669413219628, 'become': 0.004542887285986211, 'bed': 0.007921005232626186, 'behave': 0.017947115330035457, 'behaviour': 0.005913902897913882, 'big': 0.002930362622985461, 'bit': 0.0033530855558755305, 'bonus': 0.007305301529634293, 'booze': 0.010026110097409271, 'boxing': 0.007305301529634293, 'brace': 0.007582159033297317, 'called': 0.0041796140906410284, 'came': 0.002427556711352369, 'certain': 0.004763347935451558, 'changed': 0.010400393329702416, 'chap': 0.008973557665017728, 'chesterfield': 0.010026110097409271, 'children': 0.007305301529634293, 'christmans': 0.010026110097409271, 'christmas': 0.08696477174635121, 'coach': 0.002003500915026079, 'continue': 0.0046712887091496375, 'could': 0.0017292149821291779, 'coventry': 0.014610603059268586, 'day': 0.010745007987397873, 'days': 0.0031433025849829242, 'discipline': 0.006689597826642401, 'dramatically': 0.007921005232626186, 'dublin': 0.005021341691258966, 'eating': 0.008357853962025836, 'embarrassing': 0.007921005232626186, 'entire': 0.006018669413219628, 'eve': 0.0062527490972427505, 'eventually': 0.005637045394250858, 'everybody': 0.006018669413219628, 'excess': 0.008973557665017728, 'fact': 0.004584492961859315, 'feet': 0.0058159003678431, 'festivities': 0.010026110097409271, 'fight': 0.005264823957487617, 'find': 0.00811117001231549, 'football': 0.005388513810146819, 'footballer': 0.006529606600905775, 'footballers': 0.015164318066594634, 'game': 0.0024218839451399664, 'games': 0.0021920218167008803, 'gave': 0.0035747183491076735, 'get': 0.0018707308669351265, 'getting': 0.0038343198469122635, 'go': 0.004214333320625979, 'going': 0.006080410048204462, 'got': 0.002090750152591382, 'govern': 0.010026110097409271, 'grotto': 0.010026110097409271, 'ground': 0.004147644232459665, 'happy': 0.0034498387957281823, 'holding': 0.006018669413219628, 'holiday': 0.010026110097409271, 'home': 0.0022726697330249833, 'hope': 0.003490334853594669, 'hours': 0.005078650838770048, 'idiot': 0.010026110097409271, 'important': 0.003596567822104182, 'injured': 0.003913564548436542, 'kids': 0.006689597826642401, 'know': 0.0077660421974920695, 'lack': 0.005021341691258966, 'ladies': 0.008973557665017728, 'last': 0.0033534207951527516, 'leave': 0.0043504132778361924, 'leaving': 0.0047166207971369375, 'leeds': 0.0058159003678431, 'let': 0.004502391228119725, 'level': 0.0033719493037311465, 'lights': 0.008973557665017728, 'like': 0.0042475251739438925, 'local': 0.006018669413219628, 'look': 0.0032978608454446497, 'looking': 0.0031433025849829242, 'lot': 0.0024688793203639045, 'love': 0.005138207969185364, 'manager': 0.002670153031129702, 'managers': 0.012505498194485501, 'manchester': 0.003210061123126359, 'massive': 0.004763347935451558, 'mentioned': 0.007921005232626186, 'millwall': 0.006689597826642401, 'minder': 0.010026110097409271, 'missed': 0.0033530855558755305, 'mobile': 0.010026110097409271, 'morning': 0.011109887321022536, 'much': 0.0026942569050734094, 'name': 0.0047166207971369375, 'need': 0.0030950918000681228, 'never': 0.011721450491941844, 'new': 0.007073281843936485, 'news': 0.004212271525096074, 'newspaper': 0.0047166207971369375, 'next': 0.002082607976652282, 'nice': 0.005200196664851208, 'night': 0.010276415938370727, 'norm': 0.010026110097409271, 'notoriety': 0.010026110097409271, 'nottingham': 0.006689597826642401, 'november': 0.004502391228119725, 'nowadays': 0.008357853962025836, 'older': 0.007921005232626186, 'one': 0.001940684521212194, 'ones': 0.006689597826642401, 'opportunity': 0.0035747183491076735, 'order': 0.0043146819310470805, 'others': 0.0116318007356862, 'outlook': 0.010026110097409271, 'paper': 0.006689597826642401, 'particular': 0.005078650838770048, 'parties': 0.007305301529634293, 'past': 0.0032272196353457443, 'people': 0.002930362622985461, 'per': 0.007582159033297317, 'period': 0.004763347935451558, 'phone': 0.014610603059268586, 'photos': 0.010026110097409271, 'picking': 0.006384876932520372, 'pictures': 0.008973557665017728, 'played': 0.0018849226727982997, 'player': 0.004526988809923051, 'players': 0.01294080478990527, 'playing': 0.0042980031949591485, 'pm': 0.016715707924051672, 'pop': 0.010026110097409271, 'pre': 0.00613120327116159, 'prior': 0.005913902897913882, 'problem': 0.003940925868408419, 'professional': 0.005138207969185364, 'prosperous': 0.008973557665017728, 'realised': 0.007582159033297317, 'reg': 0.010026110097409271, 'regarding': 0.007071221845611171, 'remember': 0.0054770541685142315, 'responsibility': 0.0062527490972427505, 'retire': 0.005637045394250858, 'ringing': 0.008973557665017728, 'said': 0.000823294911118814, 'santa': 0.010026110097409271, 'scunthorpe': 0.007921005232626186, 'season': 0.0020425393676765796, 'see': 0.0026346997746580936, 'seeing': 0.005637045394250858, 'send': 0.006384876932520372, 'sheffield': 0.004811558720366359, 'since': 0.0022362964449139087, 'smelling': 0.010026110097409271, 'snow': 0.010026110097409271, 'somebody': 0.019154630797561117, 'spirit': 0.005637045394250858, 'squad': 0.005487287535262947, 'step': 0.004912830384475858, 'strike': 0.005078650838770048, 'take': 0.009532311068621192, 'taking': 0.0033530855558755305, 'talking': 0.0046712887091496375, 'team': 0.0014268356646846875, 'tenterhooks': 0.008973557665017728, 'terms': 0.005264823957487617, 'things': 0.006489148498184845, 'think': 0.004264258132796272, 'thinking': 0.005264823957487617, 'though': 0.003490334853594669, 'time': 0.002278938768618675, 'times': 0.0034498387957281823, 'trained': 0.007582159033297317, 'training': 0.01059582158840332, 'tried': 0.004966116980828085, 'trip': 0.004811558720366359, 'trust': 0.02191590458890288, 'try': 0.0032621294986555378, 'turn': 0.004502391228119725, 'two': 0.0010922134240585048, 'unfortunately': 0.006384876932520372, 'us': 0.0021920218167008803, 'usual': 0.006868452800234643, 'wake': 0.007071221845611171, 'wants': 0.004116333593813069, 'warn': 0.008973557665017728, 'week': 0.002566183844366552, 'well': 0.0017292149821291779, 'whole': 0.0042797720677372865, 'william': 0.005913902897913882, 'wish': 0.005402965710227735, 'work': 0.003334453270892779, 'worry': 0.006868452800234643, 'would': 0.0027807375656006186, 'wright': 0.006384876932520372, 'year': 0.006402264826627484, 'years': 0.006549959152161788, 'young': 0.0038866875251278325, 'youth': 0.007071221845611171}\n"
     ]
    }
   ],
   "source": [
    "print(\"The tfidf values of the words in the final document: \\n\"+ str(final_docCollection[736]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 relevent documents with their scores for the given query are: \n",
      "\n",
      "{736: 0.08696477174635121, 61: 0.018942821568512146, 686: 0.014439433799394163, 462: 0.013015135907617187, 553: 0.01145643699652531, 409: 0.011320857860471755, 452: 0.0073023854138157505, 73: 0.005780135886464431, 76: 0.0036166823788652677}\n",
      "\n",
      "The document '736' is the most relevent for the word 'christmas'. Therefore, one final test should show that the addition of both the scores of 'years' and 'christmas' should be equal to 0.093514730898513. It passes!\n"
     ]
    }
   ],
   "source": [
    "query_RR(postings,'christmas')\n",
    "print(\"\\nThe document \\'736\\' is the most relevent for the word \\'christmas\\'. Therefore, one final test should show that the addition of both the scores of \\'years\\' and \\'christmas\\' should be equal to 0.093514730898513. It passes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 relevent documents with their scores for the given query are: \n",
      "\n",
      "{736: 0.093514730898513, 61: 0.018942821568512146, 553: 0.018934633872845753, 731: 0.016432353662440974, 686: 0.014439433799394163, 407: 0.013723723937862793, 462: 0.013015135907617187, 385: 0.012614736144904184, 733: 0.012243714493583472, 604: 0.012164209854014748}\n"
     ]
    }
   ],
   "source": [
    "query_RR(postings,'christmas years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'nonexistantword' cannot be found in any of the documents! \n",
      "Remove this word from the query and try again.\n",
      "\n",
      "The system is also robust to words which cannot be found in any of the documents!\n"
     ]
    }
   ],
   "source": [
    "query_RR(postings,'christmas years nonExistantWORD')\n",
    "print(\"\\nThe system is also robust to words which cannot be found in any of the documents!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
